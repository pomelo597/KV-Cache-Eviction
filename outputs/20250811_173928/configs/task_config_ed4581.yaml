analysis_report: false
api_key: EMPTY
api_url: null
chat_template: null
dataset_args:
  mmlu:
    dataset_id: /data1/hfhub/datasets/mmlu
    description: The MMLU (Massive Multitask Language Understanding) benchmark is
      a comprehensive evaluation suite designed to assess the performance of language
      models across a wide range of subjects and tasks. It includes multiple-choice
      questions from various domains, such as history, science, mathematics, and more,
      providing a robust measure of a model's understanding and reasoning capabilities.
    eval_split: test
    extra_params: {}
    few_shot_num: 0
    few_shot_random: false
    filters: null
    metric_list:
    - AverageAccuracy
    model_adapter: generation
    name: mmlu
    output_types:
    - multiple_choice_logits
    - generation
    pretty_name: MMLU
    prompt_template: 'Answer the following multiple choice question about {subset_name}.
      The last line of your response should be of the following format: ''Answer:
      $LETTER'' (without quotes) where LETTER is one of ABCD. Think step by step before
      answering.


      {query}'
    query_template: null
    subset_list:
    - high_school_european_history
    - business_ethics
    - clinical_knowledge
    - medical_genetics
    - high_school_us_history
    - high_school_physics
    - high_school_world_history
    - virology
    - high_school_microeconomics
    - econometrics
    - college_computer_science
    - high_school_biology
    - abstract_algebra
    - professional_accounting
    - philosophy
    - professional_medicine
    - nutrition
    - global_facts
    - machine_learning
    - security_studies
    - public_relations
    - professional_psychology
    - prehistory
    - anatomy
    - human_sexuality
    - college_medicine
    - high_school_government_and_politics
    - college_chemistry
    - logical_fallacies
    - high_school_geography
    - elementary_mathematics
    - human_aging
    - college_mathematics
    - high_school_psychology
    - formal_logic
    - high_school_statistics
    - international_law
    - high_school_mathematics
    - high_school_computer_science
    - conceptual_physics
    - miscellaneous
    - high_school_chemistry
    - marketing
    - professional_law
    - management
    - college_physics
    - jurisprudence
    - world_religions
    - sociology
    - us_foreign_policy
    - high_school_macroeconomics
    - computer_security
    - moral_scenarios
    - moral_disputes
    - electrical_engineering
    - astronomy
    - college_biology
    system_prompt: null
    tags:
    - Knowledge
    - MCQ
    train_split: train
dataset_dir: /home/jhchen/.cache/modelscope/hub/datasets
dataset_hub: modelscope
datasets:
- mmlu
debug: false
dry_run: false
eval_backend: Native
eval_batch_size: 1
eval_config: null
eval_type: checkpoint
generation_config:
  do_sample: false
  max_length: 2048
  max_new_tokens: 512
  temperature: 1.0
  top_k: 50
  top_p: 1.0
ignore_errors: false
judge_model_args: {}
judge_strategy: auto
judge_worker_num: 1
limit: 10
mem_cache: false
model: /data1/hfhub/Qwen2.5-7B-Instruct
model_args:
  precision: torch.float16
  revision: master
model_id: Qwen2.5-7B-Instruct
model_task: text_generation
outputs: null
seed: 42
stage: all
stream: false
template_type: null
timeout: null
use_cache: null
work_dir: ./outputs/20250811_173928
